{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:50:57.100521100Z",
     "start_time": "2024-02-09T17:50:57.084772300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:51:04.291031500Z",
     "start_time": "2024-02-09T17:51:03.229444300Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/cleaned_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression:\n",
    "\n",
    "Use Case: It's used for binary classification tasks. In my case, predicting 'Is Halal' as 1 (true) or 0 (false) is a classic example of binary classification.\n",
    "Output: Logistic Regression models the probability of the default class (e.g., 'Is Halal' = 1). It outputs values between 0 and 1, which are the probabilities that a given input point belongs to a particular class.\n",
    "Sigmoid Function: It uses a logistic function (sigmoid function) to squeeze the output of a linear equation between 0 and 1.\n",
    "\n",
    "Linear Regression:\n",
    "Use Case: It's used for regression tasks where the target variable is continuous. For instance, predicting the price of a house or the temperature for a day.\n",
    "Output: Linear Regression predicts a value along a continuous range.\n",
    "\n",
    "\n",
    "Since your goal is to classify an item as either Halal or not, which is a categorical outcome, Logistic Regression is the correct model to use. Linear Regression would be inappropriate as it predicts continuous values, not categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:51:07.539727100Z",
     "start_time": "2024-02-09T17:51:07.508623100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            ID                                              Title  \\\n0  ___1d7BUUHY  reel instagram sab viral video short short vid...   \n1  ___1xVwaG14  इंडियन आर्मी के जवानों को शराब क्यों पिलाया जा...   \n2  ___1y_4dOq8  sit prank epic reaction public prank onpublicprnk   \n3  ___3Si8H-iU              ai ki madad se youtubeshorts short ai   \n4  ___3tLmXbF4            زياد قرر يشتغل نادل بمطعم حرام عالزباين   \n\n                                         Description  Practicing Level  \\\n0                                                NaN                 0   \n1  video given answer government give alcohol sol...                 0   \n2  sit prank epic reaction public prank dontsithe...                 0   \n3                hidayat youtubeshorts ai short fire                 0   \n4  مقالب شكل مجموعة مقالب بالفنانين برامج الكامير...                 0   \n\n   Permissible For  Is Halal  \\\n0                0         0   \n1                0         0   \n2                0         0   \n3                0         1   \n4                0         0   \n\n                                                Tags  \n0                                                NaN  \n1  alcohol soldier drink alcohol indian army liqu...  \n2  ball seat prank best prank video comedy prank ...  \n3       ai fire hidayat short trending youtube short  \n4  ziad sahtout زياد سحتوت فنانين كاميرا خفية مقا...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Title</th>\n      <th>Description</th>\n      <th>Practicing Level</th>\n      <th>Permissible For</th>\n      <th>Is Halal</th>\n      <th>Tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>___1d7BUUHY</td>\n      <td>reel instagram sab viral video short short vid...</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>___1xVwaG14</td>\n      <td>इंडियन आर्मी के जवानों को शराब क्यों पिलाया जा...</td>\n      <td>video given answer government give alcohol sol...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>alcohol soldier drink alcohol indian army liqu...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>___1y_4dOq8</td>\n      <td>sit prank epic reaction public prank onpublicprnk</td>\n      <td>sit prank epic reaction public prank dontsithe...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>ball seat prank best prank video comedy prank ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>___3Si8H-iU</td>\n      <td>ai ki madad se youtubeshorts short ai</td>\n      <td>hidayat youtubeshorts ai short fire</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>ai fire hidayat short trending youtube short</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>___3tLmXbF4</td>\n      <td>زياد قرر يشتغل نادل بمطعم حرام عالزباين</td>\n      <td>مقالب شكل مجموعة مقالب بالفنانين برامج الكامير...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>ziad sahtout زياد سحتوت فنانين كاميرا خفية مقا...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:51:14.713136500Z",
     "start_time": "2024-02-09T17:51:14.415984600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      " Title            148\n",
      "Description    12471\n",
      "Tags           22619\n",
      "dtype: int64 \n",
      "\n",
      "Halal Distribution:\n",
      " Is Halal\n",
      "0    0.672838\n",
      "1    0.327162\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Handling Missing Values and Text Preprocessing\n",
    "# Lowercasing all textual columns and calculating missing values in one step\n",
    "text_columns = ['Title', 'Description', 'Tags']\n",
    "data[text_columns] = data[text_columns].apply(lambda x: x.str.lower())\n",
    "missing_values = data[text_columns].isnull().sum()\n",
    "\n",
    "# Check the distribution of the 'Is Halal' column\n",
    "halal_distribution = data['Is Halal'].value_counts(normalize=True)\n",
    "\n",
    "print(\"Missing Values:\\n\", missing_values, \"\\n\")\n",
    "print(\"Halal Distribution:\\n\", halal_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:51:19.817516700Z",
     "start_time": "2024-02-09T17:51:19.762412600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Practicing Level Distribution:\n",
      " Practicing Level\n",
      "0    0.836338\n",
      "1    0.150118\n",
      "3    0.009441\n",
      "2    0.004103\n",
      "Name: proportion, dtype: float64\n",
      "Permissible For Distribution:\n",
      " Permissible For\n",
      " 0    0.836338\n",
      " 1    0.144485\n",
      " 3    0.014721\n",
      " 4    0.004426\n",
      "-4    0.000029\n",
      "Name: proportion, dtype: float64\n",
      "Negative Values in 'Permissible For':\n",
      " Permissible For\n",
      "-4    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handling Missing Values\n",
    "data['Title'].fillna('no title', inplace=True)\n",
    "\n",
    "# EDA: Analyzing distributions and negative values more efficiently\n",
    "columns_to_analyze = ['Practicing Level', 'Permissible For']\n",
    "for column in columns_to_analyze:\n",
    "    distribution = data[column].value_counts(normalize=True)\n",
    "    print(f\"{column} Distribution:\\n\", distribution)\n",
    "    if column == 'Permissible For':\n",
    "        negative_values = data.loc[data[column] < 0, column].value_counts()\n",
    "        print(f\"Negative Values in '{column}':\\n\", negative_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:51:25.745245600Z",
     "start_time": "2024-02-09T17:51:24.650391100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (54400, 1000), (54400,)\n",
      "Testing set shape: (13600, 1000), (13600,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Combining feature engineering and data preparation steps for efficiency\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Limiting to 1000 features for simplicity\n",
    "X = tfidf_vectorizer.fit_transform(data['Title'])  # Vectorizing 'Title' and assigning to X\n",
    "y = data['Is Halal']  # Assigning target variable to y\n",
    "\n",
    "# Splitting the Data into Training and Testing Sets in a single step\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Displaying the shape of the training and testing sets directly\n",
    "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:51:47.068278Z",
     "start_time": "2024-02-09T17:51:46.550889400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(0.7274264705882353,\n 0.6607452339688041,\n 0.34277365700157336,\n 0.45138375018499327)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Model Building: Train Logistic Regression Model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the Test Set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:52:05.839233700Z",
     "start_time": "2024-02-09T17:51:52.408923600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(0.7014705882352941, 0.5714285714285714, 0.350561797752809, 0.4345403899721448)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Combining 'Title' and 'Description' into a single text feature for vectorization\n",
    "data['Combined_Text'] = data['Title'] + \" \" + data['Description'].fillna('')  # Fill NaN descriptions with empty strings\n",
    "\n",
    "# Reducing the feature complexity and sampling the data\n",
    "tfidf_vectorizer_reduced = TfidfVectorizer(max_features=500)  # Reducing to 500 features\n",
    "combined_text_tfidf_reduced = tfidf_vectorizer_reduced.fit_transform(data['Combined_Text'])\n",
    "\n",
    "# Sampling a subset of the data (e.g., 10% of the data)\n",
    "data_sampled = data.sample(frac=0.1, random_state=42)\n",
    "combined_text_tfidf_sampled = tfidf_vectorizer_reduced.transform(data_sampled['Combined_Text'])\n",
    "\n",
    "X_sampled = combined_text_tfidf_sampled\n",
    "y_sampled = data_sampled['Is Halal']\n",
    "\n",
    "# Splitting the Sampled Data into Training and Testing Sets\n",
    "X_train_sampled, X_test_sampled, y_train_sampled, y_test_sampled = train_test_split(\n",
    "    X_sampled, y_sampled, test_size=0.2, random_state=42, stratify=y_sampled\n",
    ")\n",
    "\n",
    "# Training the Random Forest model on the sampled data\n",
    "rf_model_sampled = RandomForestClassifier(random_state=42)\n",
    "rf_model_sampled.fit(X_train_sampled, y_train_sampled)\n",
    "\n",
    "# Predicting on the Test Set of the sampled data\n",
    "y_pred_sampled = rf_model_sampled.predict(X_test_sampled)\n",
    "\n",
    "# Evaluating the new model on the sampled data\n",
    "accuracy_sampled = accuracy_score(y_test_sampled, y_pred_sampled)\n",
    "precision_sampled = precision_score(y_test_sampled, y_pred_sampled)\n",
    "recall_sampled = recall_score(y_test_sampled, y_pred_sampled)\n",
    "f1_sampled = f1_score(y_test_sampled, y_pred_sampled)\n",
    "\n",
    "accuracy_sampled, precision_sampled, recall_sampled, f1_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:52:17.273103Z",
     "start_time": "2024-02-09T17:52:13.089990400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.69485294 0.69761029 0.70496324 0.703125   0.70220588]\n",
      "Average Cross-Validation Score: 0.7005514705882353\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Simplified hyperparameter tuning\n",
    "# Adjusting a smaller set of key parameters manually\n",
    "rf_model_simplified = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Performing cross-validation with the simplified model\n",
    "cv_scores = cross_val_score(rf_model_simplified, X_train_sampled, y_train_sampled, cv=5)\n",
    "\n",
    "# Average cross-validation score\n",
    "cv_average_score = cv_scores.mean()\n",
    "cv_scores, cv_average_score\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Average Cross-Validation Score:\", cv_average_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:52:24.305514800Z",
     "start_time": "2024-02-09T17:52:23.813021100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6955882352941176\n",
      "Precision: 0.5714285714285714\n",
      "Recall: 0.2702702702702703\n",
      "F1 Score: 0.3669724770642202\n"
     ]
    }
   ],
   "source": [
    "# Further reducing the size of the data sample and the number of features\n",
    "data_more_sampled = data.sample(frac=0.05, random_state=42)  # Reducing to 5% of the data\n",
    "tfidf_vectorizer_more_reduced = TfidfVectorizer(max_features=300)  # Reducing to 300 features\n",
    "combined_text_tfidf_more_reduced = tfidf_vectorizer_more_reduced.fit_transform(data_more_sampled['Combined_Text'])\n",
    "\n",
    "X_more_sampled = combined_text_tfidf_more_reduced\n",
    "y_more_sampled = data_more_sampled['Is Halal']\n",
    "\n",
    "# Splitting the more sampled data into training and testing sets\n",
    "X_train_more_sampled, X_test_more_sampled, y_train_more_sampled, y_test_more_sampled = train_test_split(\n",
    "    X_more_sampled, y_more_sampled, test_size=0.2, random_state=42, stratify=y_more_sampled\n",
    ")\n",
    "\n",
    "# Training Logistic Regression model on the more sampled data\n",
    "log_reg_more_sampled = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_more_sampled.fit(X_train_more_sampled, y_train_more_sampled)\n",
    "\n",
    "# Predicting on the Test Set of the more sampled data\n",
    "y_pred_more_sampled = log_reg_more_sampled.predict(X_test_more_sampled)\n",
    "\n",
    "# Evaluating the Logistic Regression model on the more sampled data\n",
    "accuracy_more_sampled = accuracy_score(y_test_more_sampled, y_pred_more_sampled)\n",
    "precision_more_sampled = precision_score(y_test_more_sampled, y_pred_more_sampled)\n",
    "recall_more_sampled = recall_score(y_test_more_sampled, y_pred_more_sampled)\n",
    "f1_more_sampled = f1_score(y_test_more_sampled, y_pred_more_sampled)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_more_sampled)\n",
    "print(\"Precision:\", precision_more_sampled)\n",
    "print(\"Recall:\", recall_more_sampled)\n",
    "print(\"F1 Score:\", f1_more_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Text Preproccsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T18:18:05.121731100Z",
     "start_time": "2024-02-09T17:52:31.935783200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shakil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Enhanced preprocessing function\n",
    "def preprocess_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = re.sub(r\"[^a-zA-Z]\", \" \", text).lower().split()\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply preprocessing to combined text\n",
    "data['Processed_Combined_Text'] = data['Combined_Text'].apply(preprocess_text)\n",
    "\n",
    "# Vectorization with TF-IDF on processed text\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500)  # Adjust max_features as needed\n",
    "X = tfidf_vectorizer.fit_transform(data['Processed_Combined_Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T18:23:27.012203200Z",
     "start_time": "2024-02-09T18:18:36.630749700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best Parameters: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Best Score: 0.7248529325429064\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "# Ensure X_train and y_train are defined and are the correct format\n",
    "if 'X_train' not in locals() or 'y_train' not in locals():\n",
    "    raise ValueError(\"X_train and y_train need to be defined before fitting the model.\")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T18:24:25.605671300Z",
     "start_time": "2024-02-09T18:24:25.504410500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                                                   ___1xVwaG14\n",
      "Title          इंडियन आर्मी के जवानों को शराब क्यों पिलाया जा...\n",
      "Description    video given answer government give alcohol sol...\n",
      "Tags           alcohol soldier drink alcohol indian army liqu...\n",
      "Name: 1, dtype: object\n",
      "\n",
      "\n",
      "ID                          _CWNbBa7QC8\n",
      "Title                        ciao italy\n",
      "Description    join channel access perk\n",
      "Tags                kamiladawn miladawn\n",
      "Name: 58966, dtype: object\n",
      "\n",
      "\n",
      "ID                                                   _33ro4bBOSk\n",
      "Title          help help short drawing cartoon story xiaolind...\n",
      "Description    tiktok instagram business email linoypeng gmai...\n",
      "Tags           kunst animation art artist cartoon draw drawin...\n",
      "Name: 16919, dtype: object\n",
      "\n",
      "\n",
      "ID                                                   _4a5Kc_Uys4\n",
      "Title          premer jadukor momtaz ashraf প্রেমের জাদুকর mu...\n",
      "Description    plz subscribe song premer jadukor প্রেমের জাদু...\n",
      "Tags           ashraf udash romantic song ashraf udash song b...\n",
      "Name: 20608, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd  # Ensure pandas is imported\n",
    "\n",
    "# Assuming video_indices is a dictionary mapping video IDs to their index in 'X'\n",
    "video_indices = {vid: idx for idx, vid in enumerate(data['ID'])}\n",
    "\n",
    "def recommend_videos(video_id, X, video_indices, data, num_recommendations=5):\n",
    "    if video_id not in video_indices:\n",
    "        return \"Video ID not found in the dataset.\"\n",
    "\n",
    "    idx = video_indices[video_id]\n",
    "    is_halal = data.loc[data['ID'] == video_id, 'Is Halal'].iloc[0]\n",
    "    filtered_data = data[data['Is Halal'] == is_halal]\n",
    "    filtered_indices = pd.Series(filtered_data.index, index=filtered_data['ID']).drop_duplicates()\n",
    "    filtered_data = filtered_data[filtered_data['ID'] != video_id]\n",
    "    filtered_X = X[filtered_indices.values]\n",
    "\n",
    "    cos_sim = cosine_similarity(X[idx:idx+1], filtered_X).flatten()\n",
    "    sorted_indices = np.argsort(cos_sim)[::-1]  # Sorted in descending order\n",
    "\n",
    "    recommendations = []\n",
    "    for index in sorted_indices[:num_recommendations]:\n",
    "        recommended_video = filtered_data.iloc[index]\n",
    "        # Check if Tags is not NaN\n",
    "        if not pd.isna(recommended_video['Tags']):\n",
    "            recommendations.append(recommended_video[['ID', 'Title', 'Description', 'Tags']])\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "# Example usage\n",
    "video_id = '___1d7BUUHY'\n",
    "recommendations = recommend_videos(video_id, X, video_indices, data)\n",
    "for rec in recommendations:\n",
    "    print(rec)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T18:24:32.931391900Z",
     "start_time": "2024-02-09T18:24:32.875986500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ID: 19796\n",
      "Title: छोटे भाई के लिए बहन ने बनाया खिलौना short viral video trending shortsvideo viralvideo\n",
      "Description: छोटे भाई के लिए बहन ने बनाया खिलौना short viral video trending shortsvideo viralvideo\n",
      "Tags: nan\n",
      "\n",
      "Video ID: 4069\n",
      "Title: प्रधानमंत्री आदर्श ग्राम योजना क्या होती है\n",
      "Description: प्रधानमंत्री आदर्श ग्राम योजना क्या होती है pradhan mantri adarsh gram yojna pmagy pmagy jmdajit reel trending foryou trendingvideos viral\n",
      "Tags: nan\n",
      "\n",
      "Video ID: 46410\n",
      "Title: हे पदार्थ मूळव्याध मुळापासून बरा करतात mulvyadh short reel swagattodkar short ाठी pile\n",
      "Description: हे पदार्थ मूळव्याध मुळापासून बरा करतात mulvyadh short reel swagattodkar short ाठी pile\n",
      "Tags: nan\n",
      "\n",
      "Video ID: 61642\n",
      "Title: makkah makkah reel short viralvideo trendingshorts short\n",
      "Description: makkah makkah reel short viralvideo trendingshorts short\n",
      "Tags: nan\n",
      "\n",
      "Video ID: 44657\n",
      "Title: damainya suasana pedesaan wonosobo short\n",
      "Description: bagi mereka yang berada di pedesaan terkadang pemandangan seperti ini adalah hal yang biasa tapi berbeda lagi bagi sebagian orang yang berada di perkotaan pemandangan seperti ini sangat luar biasa dimana alam berpadu dengan kehidupan masyarakat yang terlihat amat tenteram hal yang membuatnya mewah adalah lukisan alam yang sangat indah sehingga terbesit keinginan untuk melihat keindahannya secara nyata selamat pagi semuanya salam dari alam wonosobo footage catur khazae indoflashlight tour wisata instagram pesonaindonesia wonderfulindonesia wisataindonesia indonesianparadise wonosobo jawatengah reel indonesia\n",
      "Tags: nan\n"
     ]
    }
   ],
   "source": [
    "def recommend_halal_videos(video_id, X, video_indices, data, num_recommendations=5):\n",
    "    # Check if the video ID is in the dataset\n",
    "    if video_id not in video_indices:\n",
    "        return \"Video ID not found in the dataset.\"\n",
    "\n",
    "    # Get the index of the video in the TF-IDF matrix\n",
    "    idx = video_indices[video_id]\n",
    "\n",
    "    # Filter the dataset for Halal videos\n",
    "    filtered_data = data[data['Is Halal'] == 1]\n",
    "\n",
    "    # Update indices for the filtered data\n",
    "    filtered_indices = pd.Series(filtered_data.index, index=filtered_data['ID']).drop_duplicates()\n",
    "\n",
    "    # Ensure the video itself is not recommended\n",
    "    filtered_data = filtered_data[filtered_data['ID'] != video_id]\n",
    "\n",
    "    # Update the TF-IDF matrix for the filtered data\n",
    "    filtered_X = X[filtered_indices.values]\n",
    "\n",
    "    # Calculate cosine similarity once\n",
    "    cos_sim = cosine_similarity(X[idx:idx+1], filtered_X).flatten()\n",
    "\n",
    "    # Get the indices of the videos sorted by similarity in descending order\n",
    "    sorted_indices = np.argsort(cos_sim)[::-1]\n",
    "\n",
    "    recommendations = []\n",
    "    count = 0\n",
    "\n",
    "    # Fetch recommendations until the desired number is reached\n",
    "    for index in sorted_indices:\n",
    "        if count >= num_recommendations:\n",
    "            break\n",
    "\n",
    "        recommended_video = filtered_data.iloc[index]\n",
    "\n",
    "        # Check if the description is not nan\n",
    "        if not pd.isna(recommended_video['Description']):\n",
    "            recommendations.append({\n",
    "                \"Video ID\": recommended_video.name,  # Adding video_id to the recommendation\n",
    "                \"Title\": recommended_video['Title'],\n",
    "                \"Description\": recommended_video['Description'],\n",
    "                \"Tags\": recommended_video['Tags']\n",
    "            })\n",
    "            count += 1\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "# Example usage\n",
    "video_id = '___1d7BUUHY'  # Replace with actual video ID\n",
    "recommendations = recommend_halal_videos(video_id, X, video_indices, data)\n",
    "for rec in recommendations:\n",
    "    print(f\"Video ID: {rec['Video ID']}\")  # Displaying video_id\n",
    "    print(f\"Title: {rec['Title']}\")\n",
    "    print(f\"Description: {rec['Description']}\")\n",
    "    print(f\"Tags: {rec['Tags']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommand Halal Video even if the recieved video is Hallal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T18:24:43.170304700Z",
     "start_time": "2024-02-09T18:24:43.107359600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ID: 13641\n",
      "Title: darul uloom waqf deoband darul uloom deoband deoband trending viral trend short short video\n",
      "Description: darul uloom deoband waqf darul uloom deoband deoband darul uloom deoband waqf new video trending viral trend viralvideo video deoband shortvideo alazkardeoband hamdtvdeoband ulamaedeoband voiceofdeoband\n",
      "Tags: darul uloom deoband darul uloom deoband waqf darul uloom waqf darul uloom waqf deoband deoband deoband ulamai deoband waqf darul uloom\n",
      "\n",
      "\n",
      "\n",
      "Video ID: 38541\n",
      "Title: vfx short video\n",
      "Description: vfx short video\n",
      "Tags: mobile edits photo director vfx vineeth creation\n",
      "\n",
      "\n",
      "\n",
      "Video ID: 35882\n",
      "Title: প্রিয় শায়খ আল্লাহ আপনার সহায় হোন।। short video।।\n",
      "Description: প্রিয় শায়খ আল্লাহ আপনার সহায় হোন।। short video।।\n",
      "Tags: allama mamunul hoque short video\n",
      "\n",
      "\n",
      "\n",
      "Video ID: 15334\n",
      "Title: gulkand lassi short video gulkandlassi\n",
      "Description: gulkand lassi short video gulkandlassi mukeshgulkandlassi lassishort\n",
      "Tags: gulkand lassi gulkand lassi shot video gulkand lassi video lassi mukesh gulkand lassi mukesh lassi video\n",
      "\n",
      "\n",
      "\n",
      "Video ID: 11598\n",
      "Title: कोयले से पकी ईंटें কয়লা দিয়ে ইট পাকানো shortsvideo\n",
      "Description: कोयले से पकी ईंटें কয়লা দিয়ে ইট পাকানো shortsvideo कोयले से पकी ईंटें কয়লা দিয়ে ইট পাকানো eid bhatta short video shortsvideo shortsfeed gramerparibesh\n",
      "Tags: gramerparibesh short shortsfeed shortsvideo todayshort coal fired brick oven coal fired brick eat bhatta short video eid bhatta short eid bhatta short video eid bhatta video short eat bhatta short video short feed back short video short video youtube eid bhatta short youtube shortsfeed कोयले से पकी ईंटें कोयले से पकी ईंटें কয়লা দিয়ে ইট পাকানো shortsvideo कोयले से पाकिस्तान কয়লা দিয়ে ইট আগুন কয়লা দিয়ে ইট পাকানো কয়লা দিয়ে ইট রান্না\n"
     ]
    }
   ],
   "source": [
    "# from numpy import indices  # This import is causing the issue, it should be removed or renamed\n",
    "\n",
    "def recommend_halal_videos(video_id, X, video_indices, data, num_recommendations=5):  # Renamed 'indices' parameter to 'video_indices' to avoid conflict\n",
    "    if video_id not in video_indices:  # Changed 'indices' to 'video_indices'\n",
    "        return \"Video ID not found in the dataset.\"\n",
    "\n",
    "    idx = video_indices[video_id]  # Changed 'indices' to 'video_indices'\n",
    "    filtered_data = data[data['Is Halal'] == 1]\n",
    "    filtered_indices = pd.Series(filtered_data.index, index=filtered_data['ID']).drop_duplicates()\n",
    "    filtered_data = filtered_data[filtered_data['ID'] != video_id]\n",
    "    filtered_X = X[filtered_indices.values]\n",
    "\n",
    "    # Calculate cosine similarity once\n",
    "    cos_sim = cosine_similarity(X[idx:idx+1], filtered_X).flatten()\n",
    "    sorted_indices = np.argsort(cos_sim)[::-1]  # Sorted in descending order\n",
    "\n",
    "    recommendations = []\n",
    "    for index in sorted_indices:\n",
    "        if len(recommendations) >= num_recommendations:\n",
    "            break\n",
    "\n",
    "        recommended_video = filtered_data.iloc[index]\n",
    "        # Check if both description and tags are not NaN\n",
    "        if not pd.isna(recommended_video['Description']) and not pd.isna(recommended_video['Tags']):\n",
    "            recommendations.append({\n",
    "                \"Video ID\": recommended_video.name,  # Adding video_id to the recommendation\n",
    "                \"Title\": recommended_video['Title'],\n",
    "                \"Description\": recommended_video['Description'],\n",
    "                \"Tags\": recommended_video['Tags']\n",
    "            })\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "# Example usage\n",
    "video_id = '___1d7BUUHY'  # Replace with actual video ID\n",
    "recommendations = recommend_halal_videos(video_id, X, video_indices, data)  # Changed 'indices' to 'video_indices'\n",
    "for rec in recommendations:\n",
    "    print(f\"Video ID: {rec['Video ID']}\")  # Displaying video_id\n",
    "    print(f\"Title: {rec['Title']}\")\n",
    "    print(f\"Description: {rec['Description']}\")\n",
    "    print(f\"Tags: {rec['Tags']}\")\n",
    "    print(\"\\n\\n\")  # Two lines of space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T18:24:49.231854400Z",
     "start_time": "2024-02-09T18:24:48.998100300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All recommended videos are Halal.\n",
      "All recommended Halal data entries have Title, Description, and Tags.\n",
      "Valid Halal Entries in Recommendations: 0\n",
      "Non-Halal Entries in Recommendations: 0\n"
     ]
    }
   ],
   "source": [
    "def verify_halal_recommendations(data, recommendations):\n",
    "    # Filter data to only include IDs present in recommendations\n",
    "    recommended_data = data[data['ID'].isin([rec['Video ID'] for rec in recommendations])]\n",
    "    \n",
    "    # Check if all recommended entries are Halal\n",
    "    non_halal_in_recommendations = recommended_data[recommended_data['Is Halal'] == 0]\n",
    "    if non_halal_in_recommendations.empty:\n",
    "        print(\"All recommended videos are Halal.\")\n",
    "    else:\n",
    "        # Highlighting the presence of Non-Halal videos in the recommendations\n",
    "        print(f\"Warning: Found {len(non_halal_in_recommendations)} Non-Halal video(s) in the recommendations. This may indicate a need for model reevaluation.\")\n",
    "        # Printing IDs of Non-Halal videos\n",
    "        print(\"Non-Halal Video IDs:\", non_halal_in_recommendations['ID'].tolist())\n",
    "    \n",
    "    # Check if all halal_data entries have Title, Description, and Tags\n",
    "    halal_data_valid = recommended_data.dropna(subset=['Title', 'Description', 'Tags'])\n",
    "    if len(halal_data_valid) == len(recommended_data):\n",
    "        print(\"All recommended Halal data entries have Title, Description, and Tags.\")\n",
    "    else:\n",
    "        print(\"Some recommended Halal data entries are missing Title, Description, or Tags.\")\n",
    "    \n",
    "    return halal_data_valid, non_halal_in_recommendations\n",
    "\n",
    "# Example usage\n",
    "recommendations = recommend_halal_videos(video_id, X, video_indices, data)  # Assuming this function returns a list of recommendations\n",
    "halal_data_valid, non_halal_in_recommendations = verify_halal_recommendations(data, recommendations)\n",
    "print(f\"Valid Halal Entries in Recommendations: {len(halal_data_valid)}\")\n",
    "print(f\"Non-Halal Entries in Recommendations: {len(non_halal_in_recommendations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T18:24:54.181474300Z",
     "start_time": "2024-02-09T18:24:54.155924800Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "\n",
    "def recommend_halal_videos():\n",
    "    # Implement logic to randomly select five Halal videos\n",
    "    halal_videos = data[data['Is Halal'] == 1]\n",
    "    return random.sample(list(halal_videos['ID']), min(len(halal_videos), 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T18:24:59.338668800Z",
     "start_time": "2024-02-09T18:24:58.053272900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7295588235294118\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Printing the accuracy\n",
    "print(\"Model Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T18:25:26.995212100Z",
     "start_time": "2024-02-09T18:25:26.958319100Z"
    }
   },
   "outputs": [],
   "source": [
    "# write a pickle file name check\n",
    "import pickle\n",
    "\n",
    "# Save the model to disk\n",
    "filename = 'finalized_model.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(rf_model_sampled, file)"
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T18:25:31.936985Z",
     "start_time": "2024-02-09T18:25:31.852347600Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "# Save the TF-IDF vectorizer to a file\n",
    "# Ensure that 'tfidf_vectorizer' is the variable name of your trained TF-IDF vectorizer\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tfidf_vectorizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
